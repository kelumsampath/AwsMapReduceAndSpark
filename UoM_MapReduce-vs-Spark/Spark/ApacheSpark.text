# Import the necessary libraries
from pyspark.sql import SparkSession
import time

# Create a SparkSession
sqlContext = SparkSession.builder.appName("Assignment").getOrCreate()

# Create a DataFrame from a CSV file
df = sqlContext.read.csv("s3://mydatastorefile/DelayedFlights-updated.csv", header=True, inferSchema=True)

# Register the DataFrame as a temporary view
df.createOrReplaceTempView("delay_flights")



# Execute a Spark SQL query on the DataFrame
//Year wise carrier delay from 2003-2010

start_time = time.time()
result = sqlContext.sql("SELECT Year, avg((CarrierDelay /ArrDelay)*100) as _C1 from delay_flights GROUP BY Year").show()
end_time = time.time()
print("Execution time:", end_time - start_time, "seconds")


// Year wise NAS delay from 2003-2010

start_time = time.time()
result = sqlContext.sql("SELECT Year, avg((NASDelay /ArrDelay)*100) as _C1 from delay_flights GROUP BY Year").show()
end_time = time.time()
print("Execution time:", end_time - start_time, "seconds")

// Year wise Weather delay from 2003-2010

start_time = time.time()
result = sqlContext.sql("SELECT Year, avg((WeatherDelay /ArrDelay)*100) as _C1 from delay_flights GROUP BY Year").show()
end_time = time.time()
print("Execution time:", end_time - start_time, "seconds")

// Year wise late aircraft delay from 2003-2010

start_time = time.time()
result = sqlContext.sql("SELECT Year, avg((LateAircraftDelay /ArrDelay)*100) as _C1 from delay_flights GROUP BY Year").show()
end_time = time.time()
print("Execution time:", end_time - start_time, "seconds")

// Year wise security delay from 2003-2010

start_time = time.time()
result = sqlContext.sql("SELECT Year, avg((SecurityDelay /ArrDelay)*100) as _C1 from delay_flights GROUP BY Year").show()
end_time = time.time()
print("Execution time:", end_time - start_time, "seconds") 
